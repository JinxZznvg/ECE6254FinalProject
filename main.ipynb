{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Change the root path to the current file"
      ],
      "metadata": {
        "id": "WgBZ6Kl8z0fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h5yOu_emle6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21061e5b-bb52-41ac-c7b0-8d7ce843e39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PalmPrint\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/PalmPrint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install relative package"
      ],
      "metadata": {
        "id": "dOYnjmnC0GBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-metric-learning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCvK1HgViRsb",
        "outputId": "4e1334d1-d0bd-4281-d2c3-6219756eff44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.0.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.3/109.3 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (2.0.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.22.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.10.7)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (16.0.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "from pytorch_metric_learning import losses\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "__8xT6UXkXu1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = \"/content/drive/MyDrive/PalmPrint/iPhone\"\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "label_mapping = {}\n",
        "label_counter = 0\n",
        "\n",
        "# Iterate through the root folder and all subfolders under the iPhone directory\n",
        "for root, dirs, files in os.walk(root_path):\n",
        "    # Get all images with common extensions in the current folder\n",
        "    image_files = glob.glob(os.path.join(root, \"*.JPG\"))\n",
        "\n",
        "    # Open images and append them to the all_images list, and append their labels to the all_labels list\n",
        "    for img_file in image_files:\n",
        "        img = Image.open(img_file)\n",
        "        all_images.append(img)\n",
        "        \n",
        "        # Get the folder name\n",
        "        folder_name = os.path.basename(os.path.dirname(img_file))\n",
        "        \n",
        "        # Assign a numeric label to the folder if it's not assigned yet\n",
        "        if folder_name not in label_mapping:\n",
        "            label_mapping[folder_name] = label_counter\n",
        "            label_counter += 1\n",
        "\n",
        "        # Get the numeric label from the label_mapping dictionary\n",
        "        label = label_mapping[folder_name]\n",
        "        all_labels.append(label)\n",
        "\n",
        "print(f\"Total images found: {len(all_images)}\")\n",
        "print(f\"Total labels found: {len(all_labels)}\")\n",
        "print(f\"Label mapping: {label_mapping}\")\n"
      ],
      "metadata": {
        "id": "TTskGH_l79yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2983975a-a616-4703-9c26-db845daeb3c9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found: 3930\n",
            "Total labels found: 3930\n",
            "Label mapping: {'L_016': 0, 'L_008': 1, 'L_020': 2, 'L_037': 3, 'L_024': 4, 'L_010': 5, 'L_034': 6, 'L_001': 7, 'L_029': 8, 'L_033': 9, 'L_038': 10, 'L_014': 11, 'L_036': 12, 'L_035': 13, 'L_025': 14, 'L_012': 15, 'L_023': 16, 'L_018': 17, 'L_017': 18, 'L_021': 19, 'L_030': 20, 'L_028': 21, 'L_004': 22, 'L_002': 23, 'L_027': 24, 'L_032': 25, 'L_009': 26, 'L_019': 27, 'L_015': 28, 'L_013': 29, 'L_006': 30, 'L_005': 31, 'L_003': 32, 'L_007': 33, 'L_031': 34, 'L_026': 35, 'L_022': 36, 'L_011': 37, 'L_044': 38, 'R_007': 39, 'L_039': 40, 'R_001': 41, 'R_024': 42, 'L_070': 43, 'L_064': 44, 'R_021': 45, 'R_039': 46, 'R_023': 47, 'R_029': 48, 'R_027': 49, 'L_065': 50, 'L_052': 51, 'L_067': 52, 'L_077': 53, 'L_054': 54, 'R_032': 55, 'R_009': 56, 'R_008': 57, 'L_059': 58, 'L_040': 59, 'R_050': 60, 'R_037': 61, 'L_084': 62, 'R_004': 63, 'R_013': 64, 'R_033': 65, 'R_035': 66, 'R_047': 67, 'L_057': 68, 'R_022': 69, 'L_087': 70, 'L_058': 71, 'L_050': 72, 'R_012': 73, 'R_002': 74, 'L_074': 75, 'L_072': 76, 'L_056': 77, 'L_063': 78, 'L_091': 79, 'L_075': 80, 'R_015': 81, 'R_041': 82, 'L_062': 83, 'L_095': 84, 'R_003': 85, 'L_068': 86, 'L_045': 87, 'L_079': 88, 'L_069': 89, 'R_010': 90, 'R_036': 91, 'R_025': 92, 'L_048': 93, 'L_078': 94, 'L_083': 95, 'L_043': 96, 'R_011': 97, 'L_100': 98, 'L_080': 99, 'R_042': 100, 'L_094': 101, 'L_046': 102, 'L_098': 103, 'R_026': 104, 'L_047': 105, 'R_049': 106, 'R_031': 107, 'R_017': 108, 'L_061': 109, 'L_071': 110, 'L_081': 111, 'R_005': 112, 'R_030': 113, 'R_040': 114, 'R_018': 115, 'L_086': 116, 'R_045': 117, 'L_049': 118, 'R_034': 119, 'R_006': 120, 'L_041': 121, 'R_014': 122, 'L_093': 123, 'R_043': 124, 'L_042': 125, 'L_099': 126, 'L_089': 127, 'R_044': 128, 'R_019': 129, 'R_046': 130, 'L_053': 131, 'L_088': 132, 'L_085': 133, 'R_038': 134, 'L_092': 135, 'L_082': 136, 'L_073': 137, 'R_016': 138, 'L_060': 139, 'L_096': 140, 'L_066': 141, 'L_097': 142, 'L_076': 143, 'L_090': 144, 'L_055': 145, 'R_020': 146, 'R_028': 147, 'R_065': 148, 'R_076': 149, 'R_058': 150, 'R_084': 151, 'R_077': 152, 'R_083': 153, 'R_066': 154, 'R_072': 155, 'R_075': 156, 'R_071': 157, 'R_092': 158, 'R_094': 159, 'R_087': 160, 'R_073': 161, 'R_091': 162, 'R_052': 163, 'R_080': 164, 'R_054': 165, 'R_059': 166, 'R_098': 167, 'R_099': 168, 'R_053': 169, 'R_089': 170, 'R_055': 171, 'R_078': 172, 'R_063': 173, 'R_070': 174, 'R_095': 175, 'R_060': 176, 'R_069': 177, 'R_096': 178, 'R_086': 179, 'R_085': 180, 'R_074': 181, 'R_057': 182, 'R_067': 183, 'R_093': 184, 'R_090': 185, 'R_081': 186, 'R_062': 187, 'R_056': 188, 'R_082': 189, 'R_064': 190, 'R_068': 191, 'R_100': 192, 'R_097': 193, 'R_088': 194, 'R_061': 195, 'R_079': 196}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The rest of the imports remain the same\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(all_images, all_labels, test_size=0.2, random_state=42, stratify=all_labels)\n"
      ],
      "metadata": {
        "id": "bCI2I9ZHa2WF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),  # Convert the image to grayscale\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Repeat grayscale channel to create a 3-channel image\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Create a custom dataset with all_images and all_labels\n",
        "train_dataset = CustomDataset(train_images, train_labels, transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = CustomDataset(test_images, test_labels, transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "QmBWc6Y3iATC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model(torch.nn.Module):\n",
        "    def __init__(self, out_feature=128):\n",
        "        super(model, self).__init__()\n",
        "        # encoder\n",
        "        self.f = resnet50(pretrained=True)\n",
        "        # classifier\n",
        "        self.fc = nn.Linear(1000, out_feature, bias=True)\n",
        "\n",
        "        for param in self.f.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.fc(feature)\n",
        "        return out"
      ],
      "metadata": {
        "id": "etK_c3m-Z2Bu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = model()\n",
        "#encoder = nn.Sequential(*list(encoder.children())[:-1])\n",
        "encoder.to(device)\n",
        "\n",
        "num_classes = len(set(all_labels))\n",
        "arcface_loss = losses.ArcFaceLoss(num_classes, 128, margin=0.5, scale=64)\n",
        "arcface_loss.to(device)\n",
        "\n",
        "optimizer = optim.Adam(encoder.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79evIwGNiQFO",
        "outputId": "1d92355f-a96b-4730-b67f-f4db47be7023"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the encoder\n",
        "num_epochs = 60\n",
        "loss_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        embeddings = encoder(images)\n",
        "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
        "\n",
        "        loss = arcface_loss(embeddings, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    if epoch%10 == 0:\n",
        "        # Save the model's state_dict\n",
        "        torch.save(encoder.state_dict(), f'model_weights{epoch}.pth')\n",
        "\n",
        "    loss_list.append(total_loss/len(train_dataloader))\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_dataloader)}\")"
      ],
      "metadata": {
        "id": "6aG1tYCLpKSt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "fb5152f7-73ab-4aa4-d2f8-f6f4cb3f0129"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/60], Loss: 0.2058198457211256\n",
            "Epoch [2/60], Loss: 0.328482116907835\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f982020ed8b4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_feature_vectors(encoder, dataloader, num_classes, feature_dim):\n",
        "    class_sum = torch.zeros(num_classes, feature_dim).cuda()\n",
        "    class_count = torch.zeros(num_classes).cuda()\n",
        "    # Set the model to evaluation mode\n",
        "    encoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Encode the inputs to obtain the feature vectors\n",
        "            feature_vectors = encoder(inputs)\n",
        "\n",
        "            # Sum the feature vectors for each class\n",
        "            for i in range(num_classes):\n",
        "                mask = (labels == i)\n",
        "                class_sum[i] += feature_vectors[mask].sum(dim=0)\n",
        "                class_count[i] += mask.sum()\n",
        "\n",
        "    # Calculate the mean feature vector for each class\n",
        "    mean_feature_vectors = class_sum / class_count.view(-1, 1)\n",
        "\n",
        "    return mean_feature_vectors\n",
        "\n",
        "mean_feature = calculate_mean_feature_vectors(encoder, train_dataloader, num_classes, 128)\n",
        "mean_feature_normalized = torch.nn.functional.normalize(mean_feature, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "tmwEE4BYwvbG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_loop(encoder, dataloader):\n",
        "\n",
        "    encoder.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          embeddings = encoder(images)\n",
        "          embeddings = embeddings.view(embeddings.size(0), -1)\n",
        "          embeddings_normalized = torch.nn.functional.normalize(embeddings, dim=1)\n",
        "\n",
        "          cosine_similarity = torch.matmul(embeddings_normalized, mean_feature_normalized.t())\n",
        "          prediction = torch.argmax(cosine_similarity, dim=1)\n",
        "\n",
        "\n",
        "          total += labels.size(0)\n",
        "          correct += (prediction == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total\n",
        "for i in range(10, 70, 10):\n",
        "  encoder.load_state_dict(torch.load(f'model_weights{i}.pth'))\n",
        "  accuracy = eval_loop(encoder, test_dataloader)\n",
        "  print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fwUfcTA7Lq7",
        "outputId": "e35ebca1-6492-4762-93da-c5fb2568294e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 60.94%\n",
            "Test Accuracy: 67.81%\n",
            "Test Accuracy: 68.70%\n",
            "Test Accuracy: 70.48%\n",
            "Test Accuracy: 70.87%\n",
            "Test Accuracy: 70.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.load_state_dict(torch.load('model_weights60.pth'))\n",
        "\n",
        "encoder.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "true_labels_all = []\n",
        "predicted_labels_all = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        embeddings = encoder(images)\n",
        "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
        "        embeddings_normalized = torch.nn.functional.normalize(embeddings, dim=1)\n",
        "\n",
        "        cosine_similarity = torch.matmul(embeddings_normalized, mean_feature_normalized.t())\n",
        "        prediction = torch.argmax(cosine_similarity, dim=1)\n",
        "\n",
        "        # Store the true labels and predicted labels\n",
        "        true_labels_all.extend(labels.cpu().numpy())\n",
        "        predicted_labels_all.extend(prediction.cpu().numpy())\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (prediction == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFkAtMvSV_4e",
        "outputId": "7f33e747-e38f-40b4-a832-165459850ba2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 70.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def plot_confusion_matrix(conf_mat, labels, save_path='confusion_matrix.png'):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "\n",
        "conf_mat = confusion_matrix(true_labels_all, predicted_labels_all)\n",
        "class_report = classification_report(true_labels_all, predicted_labels_all, output_dict=True)\n",
        "\n",
        "# Assuming your classes are integers from 0 to num_classes - 1\n",
        "labels = list(range(num_classes))\n",
        "\n"
      ],
      "metadata": {
        "id": "1FL71oxQl7sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502ee85a-448e-41b0-e697-6e6b67efe1c2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create a DataFrame from the confusion matrix\n",
        "conf_mat_df = pd.DataFrame(conf_mat, index=labels, columns=labels)\n",
        "\n",
        "# Save the confusion matrix to a CSV file\n",
        "conf_mat_df.to_csv('confusion_matrix.csv', index_label='True\\Predicted')\n",
        "\n",
        "# Create a DataFrame from the classification report\n",
        "class_report_df = pd.DataFrame(class_report).transpose()\n",
        "\n",
        "# Save the classification report to a CSV file\n",
        "class_report_df.to_csv('classification_report.csv', index_label='Class')\n"
      ],
      "metadata": {
        "id": "rYkflWjBvLwn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the loss list\n",
        "loss_list_df = pd.DataFrame(loss_list, columns=['Loss'])\n",
        "\n",
        "# Save the loss list to a CSV file\n",
        "loss_list_df.to_csv('loss_list.csv', index_label='Iteration')\n"
      ],
      "metadata": {
        "id": "R76zhlxc1fNS"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}